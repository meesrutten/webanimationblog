---
title: 'Transforming an image to animating Particles with Web Audio'
undertitle: We're going to transform an image to particles and then animate those with values from Web Audio
featuredImage: ../../src/images/musical-particles-2.jpg
imgCredit: ''
date: 2020-08-19
dateCreated: 2020-08-19
datePublished: 2020-08-19
dateModified: 2020-08-19
---

import { ToC } from '../../src/components/BlogPost/ToC';

<iframe
  title={'Musical Particles'}
  loading="lazy"
  src="https://meesrutten.me/level30wizards/particle-head/"
  frameBorder={'0'}
  allow="autoplay"
></iframe>

<ToC mdx={props.mdx} slugger={props.slugger} />

At my company [Level30Wizards](https://level30wizards.com), we experiment with web techniques to learn how we can implement those for clients.
Check it out on [our website](https://level30wizards.com/experiments/musical-particles/)
Disclaimer: This article contains a lot of WebGL shader code, which I, as of today, don't understand.

## Tech stack

- HTML
- CSS
- [P5js](https://p5js.org/) to get audio values from audio clips
- WebPack for compiling
- ThreeJS / WebGL for creating and animating particles

### Libraries

- [ControlKit](https://github.com/brunoimbrizi/controlkit.js) - GUI
- [gsap](https://greensock.com?ref=68708) - animation platform
- [glslify](https://github.com/glslify/glslify) - module system for GLSL
- [stats.js](https://github.com/mrdoob/stats.js/) - performance monitor
- [Three.js](https://github.com/mrdoob/three.js/) - WebGL library

## Goal

We want to transform a photo into particles and then animate those particles based on values we get from audio.

## Threejs Scene

First of all, we create a new scene with Three.js

```js
// scene
this.scene = new THREE.Scene();

// camera
this.camera = new THREE.PerspectiveCamera(
  100,
  window.innerWidth / window.innerHeight,
  1,
  1000
);
this.camera.position.z = 300;

// renderer
this.renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });

// clock
this.clock = new THREE.Clock(true);
```

Next to that we add some standard resize functions.
Then, we initialize our particle function.

```js
initParticles() {
  this.particles = new Particles(this);
  this.scene.add(this.particles.container);
}

// Create a draw function
draw() {
  this.renderer.render(this.scene, this.camera);
}

// Create an update function
update() {
  const delta = this.clock.getDelta();

  if (this.particles) this.particles.update(delta);
}
```

## Particles

Classes?!
Well, this project is an experiment. So I combined work from other people into a new one.
#FullStackOverflowDeveloper

```js
import * as THREE from "three";
import { gsap } from "gsap";

const glslify = require("glslify");

export default class Particles {
  constructor(webgl) {
    this.webgl = webgl;
    this.container = new THREE.Object3D();
    this.mapBass = 0;
    this.mapTremble = 0;
    this.mapMid = 0;
    this.mapLowMid = 0;
    this.mapHighMid = 0;
    this.root = document.documentElement;
    this.playPauseButton = document.querySelector(
      '[aria-label="toggle-music"]'
    );
  }
```

## Initialize particles

This code will initialize the particle animation, load the audio file and gathers the audio values.

```js
init(src) {
  gsap.fromTo(
    ".loading",
    {
      autoAlpha: 1,
    },
    {
      duration: 0.5,
      autoAlpha: 0,
    }
  );

  const loader = new THREE.TextureLoader();
  loader.load(src, (texture) => {
    this.texture = texture;
    this.texture.minFilter = THREE.LinearFilter;
    this.texture.magFilter = THREE.LinearFilter;
    this.texture.format = THREE.RGBFormat;

    this.width = texture.image.width;
    this.height = texture.image.height;

    this.initPoints(true);
    this.resize();
    this.hide();
  });

  const s = (p) => {
    let audio, fft;

    p.preload = () => {
      audio = p.loadSound("/audio/music-file.mp3", () => {
        this.playPauseButton.addEventListener("click", () => {
          if (audio.isPlaying()) {
            audio.pause();
            gsap.to("#pause", {
              autoAlpha: 0,
              duration: 0.15,
            });
            gsap.to("#play", {
              autoAlpha: 1,
              duration: 0.15,
              delay: 0.15,
            });
          } else {
            audio.loop();
            gsap.to("#play", {
              autoAlpha: 0,
              duration: 0.15,
            });
            gsap.to("#pause", {
              autoAlpha: 1,
              duration: 0.15,
              delay: 0.15,
            });
          }
        });
      });
    };
    p.setup = () => {
      fft = new p5.FFT();
      const allowButton = document.querySelector("#allow-music");

      allowButton.addEventListener("click", () => {
        audio.loop();
        this.show();
        gsap.to([allowButton, ".container img"], {
          autoAlpha: 0,
          duration: 0.5,
        });
      });
    };
    p.draw = () => {
      fft.analyze();

      const bass = fft.getEnergy("bass");
      const treble = fft.getEnergy("treble");
      const lowMid = fft.getEnergy("lowMid");
      const mid = fft.getEnergy("mid");
      const highMid = fft.getEnergy("highMid");

      this.mapBass = p.map(bass, 0, 255, 0, 2.0);
      this.mapTremble = p.map(treble, 0, 255, 0, 2.0);
      this.mapLowMid = p.map(lowMid, 0, 255, 0, 2.0);
      this.mapMid = p.map(mid, 0, 255, 0, 2.0);
      this.mapHighMid = p.map(highMid, 0, 255, 0, 2.0);
    };
  };
  new p5(s);
}
```

## Image to particles

This code was written by [Bruno Imbrizi](http://brunoimbrizi.com/)

```js
initPoints(discard) {
  this.numPoints = this.width * this.height;

  let numVisible = this.numPoints;
  let threshold = 0;
  let originalColors;

  if (discard) {
    // discard pixels darker than threshold #22
    numVisible = 0;
    threshold = 34;

    const img = this.texture.image;
    const canvas = document.createElement("canvas");
    const ctx = canvas.getContext("2d");

    canvas.width = this.width;
    canvas.height = this.height;
    ctx.scale(1, -1);
    ctx.drawImage(img, 0, 0, this.width, this.height * -1);

    const imgData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    originalColors = Float32Array.from(imgData.data);

    for (let i = 0; i < this.numPoints; i++) {
      if (originalColors[i * 4 + 0] > threshold) numVisible++;
    }
  }

  const uniforms = {
    uTime: { value: 0 },
    uRandom: { value: 1.0 },
    uDepth: { value: 100.0 },
    uSize: { value: 1.58 },
    uTextureSize: { value: new THREE.Vector2(this.width, this.height) },
    uTexture: { value: this.texture },
    uTouch: { value: null },
    // This variable was added to use audio values in the vertex shader
    music: { value: null },
  };

  const material = new THREE.RawShaderMaterial({
    uniforms,
    vertexShader: glslify(require("../../../shaders/particle.vert")),
    fragmentShader: glslify(require("../../../shaders/particle.frag")),
    depthTest: false,
    transparent: true,
  });

  const geometry = new THREE.InstancedBufferGeometry();

  // positions
  const positions = new THREE.BufferAttribute(new Float32Array(4 * 3), 3);
  positions.setXYZ(0, -0.5, 0.5, 0.0);
  positions.setXYZ(1, 0.5, 0.5, 0.0);
  positions.setXYZ(2, -0.5, -0.5, 0.0);
  positions.setXYZ(3, 0.5, -0.5, 0.0);
  geometry.addAttribute("position", positions);

  // uvs
  const uvs = new THREE.BufferAttribute(new Float32Array(4 * 2), 2);
  uvs.setXYZ(0, 0.0, 0.0);
  uvs.setXYZ(1, 1.0, 0.0);
  uvs.setXYZ(2, 0.0, 1.0);
  uvs.setXYZ(3, 1.0, 1.0);
  geometry.addAttribute("uv", uvs);

  // index
  geometry.setIndex(
    new THREE.BufferAttribute(new Uint16Array([0, 2, 1, 2, 3, 1]), 1)
  );

  const indices = new Uint16Array(numVisible);
  const offsets = new Float32Array(numVisible * 3);
  const angles = new Float32Array(numVisible);

  for (let i = 0, j = 0; i < this.numPoints; i++) {
    if (discard && originalColors[i * 4 + 0] <= threshold) continue;

    offsets[j * 3 + 0] = i % this.width;
    offsets[j * 3 + 1] = Math.floor(i / this.width);

    indices[j] = i;

    angles[j] = Math.random() * Math.PI;

    j++;
  }

  geometry.addAttribute(
    "pindex",
    new THREE.InstancedBufferAttribute(indices, 1, false)
  );
  geometry.addAttribute(
    "offset",
    new THREE.InstancedBufferAttribute(offsets, 3, false)
  );
  geometry.addAttribute(
    "angle",
    new THREE.InstancedBufferAttribute(angles, 1, false)
  );

  this.object3D = new THREE.Mesh(geometry, material);
  this.container.add(this.object3D);
}
```

Cool, so now the image will be transformed to particles.

## Updating Vertex Shader variables with ThreeJS

Now, we need to update values in the Vertex Shader for all the particles to animate.
We do this by utilizing a RequestAnimationFrame that updates the WebGL values based on your refresh rate.

```js
update(delta) {
  if (!this.object3D) return;
  if (this.touch) this.touch.update();
  this.object3D.material.uniforms.music.value = Math.max(1, this.mapBass);

  this.object3D.material.uniforms.uTime.value += delta;
}
```

## Vertex Shader

This is where it gets magical for me. I don't have a lot of experience with WebGL, Vertex shaders or Fragment shaders.
But after tinkering a bit, I was able to use the music variable in the animation.

```js
// @author brunoimbrizi / http://brunoimbrizi.com

precision highp float;

attribute float pindex;
attribute vec3 position;
attribute vec3 offset;
attribute vec2 uv;
attribute float angle;

uniform mat4 modelViewMatrix;
uniform mat4 projectionMatrix;

// Create music variable
uniform float music;

uniform float uTime;
uniform float uRandom;
uniform float uDepth;
uniform float uSize;
uniform vec2 uTextureSize;
uniform sampler2D uTexture;
uniform sampler2D uTouch;

varying vec2 vPUv;
varying vec2 vUv;

#pragma glslify: snoise2 = require(glsl-noise/simplex/2d)

float random(float n) {
	return fract(sin(n) * 43758.5453123);
}

void main() {
	vUv = uv;

	// particle uv
	vec2 puv = offset.xy / uTextureSize;
	vPUv = puv;

	// pixel color
	vec4 colA = texture2D(uTexture, puv);
	float grey = colA.r * 0.21 + colA.g * 0.71 + colA.b * 0.07;

	// displacement
	vec3 displaced = offset;
	// randomise and utilize music variable
	displaced.xy += vec2(random(pindex) - 0.25, random(offset.x + pindex) - 0.25) * uRandom * (music * 0.2);
	float rndz = (random(pindex) + snoise_1_2(vec2(pindex * 0.1, music * 0.1)));
	displaced.z += rndz * (random(pindex) * uDepth) * (music * 6.0);
	// center
	displaced.xy -= uTextureSize * 0.5;

	// touch
	float t = texture2D(uTouch, puv).r;
	displaced.z += t * 20.0 * rndz;
	displaced.x += cos(angle) * t * 20.0 * rndz;
	displaced.y += sin(angle) * t * 20.0 * rndz;

	// particle size
	float psize = (snoise_1_2(vec2(uTime* music, pindex) * 0.5) + 2.0) * music * 0.5;
	psize *= max(grey, 0.2);
	psize *= uSize;

	// final position
	vec4 mvPosition = modelViewMatrix * vec4(displaced, 1.0);
	mvPosition.xyz += position * psize;
	vec4 finalPosition = projectionMatrix * mvPosition;

	gl_Position = finalPosition;
}
```

## Fragment shader

To give some texture to the particles we add a fragment shader.

```glsl
// @author brunoimbrizi / http://brunoimbrizi.com

precision highp float;

uniform sampler2D uTexture;
uniform float uTime;

varying vec2 vPUv;
varying vec2 vUv;

void main() {
	vec4 color = vec4(0.0);
	vec2 uv = vUv;
	vec2 puv = vPUv;

	// pixel color
	vec4 colA = texture2D(uTexture, puv);

	// greyscale
	float grey = colA.r * 0.21 + colA.g * 0.71 + colA.b * 0.07;
	vec4 colB = vec4(grey, grey, grey, 1.0);

	// circle
	float border = 0.3;
	float radius = 0.5;
	float dist = radius - distance(uv, vec2(0.5));
	float t = smoothstep(0.0, border, dist);

    uv = 2.0 * uv - 1.0;

    // Double the speed
    float wave = sin(uTime * 2.0);

    // Scale to make the circle bigger so it reaches the far edges
    float circle = (uv.x * uv.x + uv.y * uv.y) * 0.2;
    vec4 color1 = vec4(0.1, 0.2, 0.8, 1.0); // Red
    vec4 color2 = vec4(0.2, 0.1, 0.8, 1.0); // Blue

	color = mix(color1, color2, circle + wave);
	// final color
	// color = colB;
	color.a = t;

	gl_FragColor = color;
}
```

## Finalize

Ofcourse, more goes into the experiment than all this code. But I don't think anyone will learn from the rest of it.
By now I learned how to use ThreeJS to update variables in shaders, in an inefficient way.
I understand how transforming an image to particles works, how you update a vertex shader and how you combine audio values with web animation.

## Demo

Have a look at the demo to see how it all comes together.
[GitHub Repo](https://github.com/Level30Wizards/particle-audio-animation)

<iframe
  title={'Musical Particles'}
  loading="lazy"
  src="https://meesrutten.me/level30wizards/particle-head/"
  frameBorder={'0'}
  allow="autoplay"
></iframe>

---

### Thanks for reading!

I hope someone somewhere learned something via this post! If you did, please consider sharing the article.
If you have some web animation concept or library you want to learn about, let me know!
